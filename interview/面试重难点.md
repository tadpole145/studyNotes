---
title: java基础知识面试总结
date: 2021-09-18 14:24:36
author: 小蝌蚪
img: 
coverImg: 
top: false
cover: false
toc: true
mathjax: false
summary: 介绍java面试中经常问到的知识点
tags: [面试]
categories: [基础知识, java]
comments: false
---







## java知识点专题总结





### HashMap

#### HashMap的底层实现

在 JDK 1.7 中 HashMap 是以数组加链表的形式组成的，数组记录了每个链表的第一个节点. JDK 1.8 之后新增了红黑树的组成结构，当链表大于 8 并且数组容量大于 64 时，链表结构会转换成红黑树结构,数组中的元素被称为哈希桶. 

![image-20210720163456580](https://gitee.com/tadpole145/images/raw/main/20210720163504.png)

哈希桶,也就是数组中的元素,定义如下:

```java
static class Node<K,V> implements Map.Entry<K,V> {

    final int hash;

    final K key;

    V value;

    Node<K,V> next;  //下一个节点元素



    Node(int hash, K key, V value, Node<K,V> next) {

        this.hash = hash;

        this.key = key;

        this.value = value;

        this.next = next;

    }



    public final K getKey()        { return key; }

    public final V getValue()      { return value; }

    public final String toString() { return key + "=" + value; }



    public final int hashCode() {

        return Objects.hashCode(key) ^ Objects.hashCode(value);

    }



    public final V setValue(V newValue) {

        V oldValue = value;

        value = newValue;

        return oldValue;

    }



    public final boolean equals(Object o) {

        if (o == this)

            return true;

        if (o instanceof Map.Entry) {

            Map.Entry<?,?> e = (Map.Entry<?,?>)o;

            if (Objects.equals(key, e.getKey()) &&

                Objects.equals(value, e.getValue()))

                return true;

        }

        return false;

    }

}

```

此外,类初始化的时候,给了一些默认的属性值

```java
// HashMap 初始化长度
static final int DEFAULT_INITIAL_CAPACITY = 1 << 4; //  16

// HashMap 最大长度
static final int MAXIMUM_CAPACITY = 1 << 30; // 1073741824

// 默认的加载因子 (扩容因子)
static final float DEFAULT_LOAD_FACTOR = 0.75f;

// 当链表长度大于此值且数组容量大于 64 时
static final int TREEIFY_THRESHOLD = 8;

// 转换链表的临界值，当元素小于此值时，会将红黑树结构转换成链表结构
static final int UNTREEIFY_THRESHOLD = 6;

// 最小树容量
static final int MIN_TREEIFY_CAPACITY =64
```



 HashMap源码中主要的方法有get(Object key), put(K key, V value), resize()方法

#### put方法原理及知识点

##### 源码

```java
public V put(K key, V value) {

    // 对 key 进行哈希操作

    return putVal(hash(key), key, value, false, true);

}

final V putVal(int hash, K key, V value, boolean onlyIfAbsent,

               boolean evict) {

    Node<K,V>[] tab; Node<K,V> p; int n, i;

    // 哈希表为空则创建表

    if ((tab = table) == null || (n = tab.length) == 0)

        n = (tab = resize()).length;

    // 根据 key 的哈希值计算出要插入的数组索引 i

    if ((p = tab[i = (n - 1) & hash]) == null)

        // 如果 table[i] 等于 null，则直接插入

        tab[i] = newNode(hash, key, value, null);

    else {

        Node<K,V> e; K k;

        // 如果 key 已经存在了，直接覆盖 value

        if (p.hash == hash &&

            ((k = p.key) == key || (key != null && key.equals(k))))

            e = p;

        // 如果 key 不存在，判断是否为红黑树

        else if (p instanceof TreeNode)

            // 红黑树直接插入键值对

            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);

        else {

            // 为链表结构，循环准备插入

            for (int binCount = 0; ; ++binCount) {

                // 下一个元素为空时

                if ((e = p.next) == null) {

                    p.next = newNode(hash, key, value, null);

                    // 转换为红黑树进行处理

                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st

                        treeifyBin(tab, hash);

                    break;

                }

                //  key 已经存在直接覆盖 value

                if (e.hash == hash &&

                    ((k = e.key) == key || (key != null && key.equals(k))))

                    break;

                p = e;

            }

        }

        if (e != null) { // existing mapping for key

            V oldValue = e.value;

            if (!onlyIfAbsent || oldValue == null)

                e.value = value;

            afterNodeAccess(e);

            return oldValue;

        }

    }

    ++modCount;

    // 超过最大容量，扩容

    if (++size > threshold)

        resize();

    afterNodeInsertion(evict);

    return null;

}

```

put 流程简化如下:

1. 对 Key 求 Hash 值，然后再计算下标
2. 如果没有碰撞，直接放入桶中（碰撞的意思是计算得到的 Hash 值相同，需要放到同一个 bucket 中）
3. 如果碰撞了，以链表的方式链接到后面
4. 如果链表长度超过阀值（TREEIFY THRESHOLD==8并且容量超过MIN_TREEIFY_CAPACITY=64），就把链表转成红黑树，链表长度低于6，就把红黑树转回链表
5. 如果节点已经存在就替换旧值
6. 如果桶满了（容量16*加载因子0.75），就需要 resize（扩容2倍后重排）

![img](https://s0.lgstatic.com/i/image3/M01/73/D9/CgpOIF5rDYmATP43AAB3coc0R64799.png)



##### 如何获取待插入元素的数组索引

jdk8中,需要通过如下代码获取到待存入数据的索引

```java
// 找到元素在数组中的位置，n 为数组长度。
i = (n - 1) & hash

// 计算 Key 的 Hash 值 
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>>16);
}
//异或运算^:  相同则为0,不同则为1
```

###### 为什么 HashMap 数组的长度要是 2 的整数幂？

 **为了 减少哈希冲突并加快运算效率**

   hash是散列的意思, 我们希望任意一个 Key 落在数组中的位置是足够散列的，这样可以减少 Hash 碰撞的概率. 我们根据hash值得到的是一个int数,放置在一个16位长的数组中,如果想让它尽可能的散列分布,最简单的办法是 hash%n, 这样得到的余数就是数组的索引, 但是这样计算太低效了,由于计算机是二进制计算的,我们可以通过二进制的==与运算==,取其hash值的后几位就可以高效的得到散列值(也就是索引), 如: 

```java
    00100100 10100101 11000100 00100101    // Hash 值 
&   00000000 00000000 00000000 00001111    // 16 - 1 = 15
----------------------------------
    00000000 00000000 00000000 00000101    // 高位全部归零，只保留末四位。与运算不改变原hash值的低位数,这样散列程度只受参与运算的hash值影响.

```

假设是其它容量值,进行与运算的结果碰撞的概率就比较大, 比如当容量是10时,n-1=9 :

![img](https://gitee.com/tadpole145/images/raw/main/20210720173626.jpeg)

总的来说,因为计算机采用二进制,使用2的整数幂,在进行运算时候能大大提升效率.



###### HashMap中的hash算法是如何降低碰撞率的

上面我们已经知道,通过与运算取最后几位值,得到索引.但是int型的hash值,末尾数重复的太多了,为了解决这类问题，HashMap 想了一种办法（ *扰动* ）：将 Hash 值的高 16 位右移并与原 Hash 值取异或运算（^），混合高 16 位和低 16 位的值，得到一个更加散列的低 16 位的 Hash 值。如：

```java
00000000 00000000 00000000 00000101 // H1
00000000 00000000 00000000 00000000 // H1 >>> 16
00000000 00000000 00000000 00000101 // hash = H1 ^ (H1 >>> 16) = 5

00000000 11111111 00000000 00000101 // H2
00000000 00000000 00000000 11111111 // H2 >>> 16
00000000 00000000 00000000 11111010 // hash = H2 ^ (H2 >>> 16) = 250
//经过与运算求索引
index1 = (n - 1) & H1 = (16 - 1) & 5 = 5
index2 = (n - 1) & H2 = (16 - 1) & 250 = 10    
```



##### jdk1.8为啥引入红黑树, 为啥不直接全部变为红黑树, 转化成红黑树的条件是什么

同一个链表上的节点，它们的hash值计算出来都是一样的。但是如果hash冲突比较多的时候，生成的链表也会拉得比较长，这个时候检索起来就会退化成遍历操作，性能就比较低了。在Java 8中为了改善这种情况，引入了红黑树。红黑树是一种高级的平衡二叉树结构，其能保证查找、插入、删除的时间复杂度最坏为O(logn), 可以有效的解决链表过长时操作比较慢的问题。 

之所以选择红黑树是为了解决二叉查找树的缺陷：二叉查找树在特殊情况下会变成一条线性结构（这就跟原来使用链表结构一样了，造成层次很深的问题），遍历查找会非常慢。而红黑树在插入新数据后可能需要通过左旋、右旋、变色这些操作来保持平衡。引入红黑树就是为了查找数据快，解决链表查询深度的问题。我们知道红黑树属于平衡二叉树，为了保持“平衡”是需要付出代价的，但是该代价所损耗的资源要比遍历线性链表要少。所以当长度大于8的时候，会使用红黑树；如果链表长度很短的话，根本不需要引入红黑树，引入反而会慢。

当链表长度＞=8,且数组长度＞64时,才会转化成红黑树.只有在节点较多时,红黑树才能发挥它的优势.另外,如果满足链表长度＞8,而数组长度＜64,会被认为是桶分配的太少才产生那么多冲突的。那么此时应该选择扩容操作，以此来降低hash冲突的产生。等到数组的长度大于等于MIN_TREEIFY_CAPACITY的时候，如果当前链表的长度还是8的话，才会去转化成红黑树。

##### 说说你对红黑树的见解？

1.  每个节点非红即黑
2. 根节点总是黑色的
3. 如果节点是红色的，则它的子节点必须是黑色的（反之不一定）
4. 每个叶子节点都是黑色的空节点（NIL节点）
5. 从根节点到叶节点或空子节点的每条路径，必须包含相同数目的黑色节点（即相同的黑色高度）



##### 链表转为红黑树的阈值是8，而红黑树退化成链表的阈值是6,为什么不一致呢

如果这两个值都为8的话，而当前链表的节点数量为7，此时一个新的节点进来了，计算出hash值和这七个节点的hash值相同，即发生了hash冲突。于是就会把这个节点挂在第七个节点的后面，但是此时已经达到了变成红黑树的阈值了（MIN_TREEIFY_CAPACITY条件假定也满足），于是就转成红黑树。

但是此时调用了一次remove操作需要删掉这个新加的节点，删掉之后当前红黑树的节点数量就又变成了7，于是就退化成了链表。然后此时又新加了一个节点，正好又要挂在第七个节点的后面，于是就又变成红黑树，然后又要remove，又退化成链表…可以看到在这种场景下，会不断地出现链表和红黑树之间的相互转换，这个性能是很低的，因为大部分的执行时间都花费在了转换数据结构上面

所以为了避免这种情况的发生，将两个阈值错开一些，以此来尽量避免在阈值点附近可能存在的、频繁地做转换数据结构操作而导致性能变低的情况出现。



#### get方法原理及其相关知识点

##### 源码

```java
public V get(Object key) {

    Node<K,V> e;

    // 对 key 进行哈希操作

    return (e = getNode(hash(key), key)) == null ? null : e.value;

}

final Node<K,V> getNode(int hash, Object key) {

    Node<K,V>[] tab; Node<K,V> first, e; int n; K k;

    // 非空判断

    if ((tab = table) != null && (n = tab.length) > 0 &&

        (first = tab[(n - 1) & hash]) != null) {

        // 判断第一个元素是否是要查询的元素

        if (first.hash == hash && // always check first node

            ((k = first.key) == key || (key != null && key.equals(k))))

            return first;

        // 下一个节点非空判断

        if ((e = first.next) != null) {

            // 如果第一节点是树结构，则使用 getTreeNode 直接获取相应的数据

            if (first instanceof TreeNode)

                return ((TreeNode<K,V>)first).getTreeNode(hash, key);

            do { // 非树结构，循环节点判断

                // hash 相等并且 key 相同，则返回此节点

                if (e.hash == hash &&

                    ((k = e.key) == key || (key != null && key.equals(k))))

                    return e;

            } while ((e = e.next) != null);

        }

    }

    return null;

}

```

get 方法流程总结:

- 计算 key 的 hash 值。
- 如果存储数组不为空，且计算得到的位置上的元素不为空。继续，否则，返回 Null。
- 如果获取到的元素的 key 值相等，说明查找到了，返回元素。
- 如果获取到的元素的 key 值不相等，查找 next 节点的元素。
	- 如果元素是红黑树，在红黑树中查找。
	- 不是红黑树，遍历 next 节点查找，找到则返回。



#### resize()方法及其知识点

##### 源码

```java
static final int MAXIMUM_CAPACITY = 1 << 30;
static final float DEFAULT_LOAD_FACTOR = 0.75f;

final Node<K,V>[] resize() {

    // 扩容前的数组

    Node<K,V>[] oldTab = table;

    // 扩容前的数组的大小和阈值

    int oldCap = (oldTab == null) ? 0 : oldTab.length;

    int oldThr = threshold;

    // 预定义新数组的大小和阈值

    int newCap, newThr = 0;

    if (oldCap > 0) {

        // 超过最大值就不再扩容了

        if (oldCap >= MAXIMUM_CAPACITY) {

            threshold = Integer.MAX_VALUE;

            return oldTab;

        }

        // 扩大容量为当前容量的两倍，但不能超过 MAXIMUM_CAPACITY

        else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&

                 oldCap >= DEFAULT_INITIAL_CAPACITY)

            newThr = oldThr << 1; // double threshold

    }

    // 当前数组没有数据，使用初始化的值

    else if (oldThr > 0) // initial capacity was placed in threshold

        newCap = oldThr;

    else {               // zero initial threshold signifies using defaults

        // 如果初始化的值为 0，则使用默认的初始化容量

        newCap = DEFAULT_INITIAL_CAPACITY;

        newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);

    }

    // 如果新的容量等于 0

    if (newThr == 0) {

        float ft = (float)newCap * loadFactor;

        newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?

                  (int)ft : Integer.MAX_VALUE);

    }

    threshold = newThr; 

    @SuppressWarnings({"rawtypes","unchecked"})

    Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];

    // 开始扩容，将新的容量赋值给 table

    table = newTab;

    // 原数据不为空，将原数据复制到新 table 中

    if (oldTab != null) {

        // 根据容量循环数组，复制非空元素到新 table

        for (int j = 0; j < oldCap; ++j) {

            Node<K,V> e;

            if ((e = oldTab[j]) != null) {

                oldTab[j] = null;

                // 如果链表只有一个，则进行直接赋值

                if (e.next == null)

                    newTab[e.hash & (newCap - 1)] = e;

                else if (e instanceof TreeNode)

                    // 红黑树相关的操作

                    ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);

                else { // preserve order

                    // 链表复制，JDK 1.8 扩容优化部分
					// 把当前index对应的链表分成两个链表，减少扩容的迁移量
                    Node<K,V> loHead = null, loTail = null;

                    Node<K,V> hiHead = null, hiTail = null;

                    Node<K,V> next;

                    do {

                        next = e.next;

                        // 原索引

                        if ((e.hash & oldCap) == 0) {

                            if (loTail == null)

                                loHead = e;

                            else

                                loTail.next = e;

                            loTail = e;

                        }

                        // 原索引 + oldCap

                        else {

                            if (hiTail == null)

                                hiHead = e;

                            else

                                hiTail.next = e;

                            hiTail = e;

                        }

                    } while ((e = next) != null);

                    // 将原索引放到哈希桶中

                    if (loTail != null) {

                        loTail.next = null;

                        newTab[j] = loHead;

                    }

                    // 将原索引 + oldCap 放到哈希桶中

                    if (hiTail != null) {

                        hiTail.next = null;

                        newTab[j + oldCap] = hiHead;

                    }

                }

            }

        }
    }

    return newTab;
}

```

resize方法执行流程如下:

1. 记录下扩容前的数组,及其大小和阈值, 初始化新数组及其阈值
2. 对原数组的大小及其阈值进行校验
	1. oldCap超过最大值就不再扩容了
	2. 扩大容量为当前容量的两倍，但不能超过 MAXIMUM_CAPACITY
	3. 如果初始化的值为 0，则使用数组默认的初始化容量16
3. 开始扩容，创建一个长度为newCap的newTab.
4. 根据oldCap遍历数组，复制非空元素到新 table
	1. 如果链表只有一个，则进行直接赋值, 重新计算在新数组中的索引e.hash & (newCap - 1)
	2. 如果是红黑树,则执行红黑树相关操作
	3. 如果是链表,则根据e.hash & oldCap 得到的结果来决定是否移动索引位置



##### 什么是加载因子？加载因子为什么是 0.75？

加载因子也叫扩容因子或负载因子，用来判断什么时候进行扩容的，假如加载因子是 0.5，HashMap 的初始化容量是 16，那么当 HashMap 中有 16*0.5=8 个元素时，HashMap 就会进行扩容。

那加载因子为什么是 0.75 而不是 0.5 或者 1.0 呢？

这其实是出于容量和性能之间平衡的结果：

当加载因子设置比较大的时候，扩容的门槛就被提高了，扩容发生的频率比较低，占用的空间会比较小，但此时发生 Hash 冲突的几率就会提升，因此需要更复杂的数据结构来存储元素，这样对元素的操作时间就会增加，运行效率也会因此降低；

而当加载因子值比较小的时候，扩容的门槛会比较低，因此会占用更多的空间，此时元素的存储就比较稀疏，发生哈希冲突的可能性就比较小，因此操作性能会比较高。

所以综合了以上情况就取了一个 0.5 到 1.0 的平均数 0.75 作为加载因子。



##### 如何理解jdk1.8中的扩容后的链表移动机制

正常情况下，计算节点在table中的下标的方法是：hash&(oldTable.length-1)，扩容之后，table长度翻倍，计算table下标的方法是hash&(newTable.length-1)，也就是hash&(oldTable.length*2-1)，于是我们有了这样的结论：这新旧两次计算下标的结果，要不然就相同，要不然就是新下标等于旧下标加上旧数组的长度。

举例:  假设table原长度是16，扩容后长度32，那么一个hash值在扩容前后的table下标是这么计算的：

![img](https://gitee.com/tadpole145/images/raw/main/20210721154935.jpeg)

hash和新旧table按位与的结果，最后4位显然是相同的，唯一可能出现的区别就在第5位，也就是hash值的b所在的那一位，如果b所在的那一位是0，那么新table按位与的结果和旧table的结果就相同，反之如果b所在的那一位是1，则新table按位与的结果就比旧table的结果多了10000（二进制），而这个二进制10000就是旧table的长度16。

换言之，hash值的新散列下标是不是需要加上旧table长度，只需要看看hash值第5位是不是1就行了，位运算的方法就是hash值和10000（也就是旧table长度）来按位与，其结果只可能是10000或者00000。

所以,源码使用e.hash & oldCap，就是用于计算位置b到底是0还是1用的，只要其结果是0，则新散列下标就等于原散列下标，否则新散列坐标要在原散列坐标的基础上加上原table长度。

![img](https://s0.lgstatic.com/i/image3/M01/73/D9/Cgq2xl5rDYmAXoWFAAArXO_oe8c713.png)

##### HashMap扩容时候的死锁问题

在Java 7的HashMap源码中，transfer方法是用来做扩容时的迁移数据操作的。其实现就是通过遍历链表中的每一个节点，重新rehash实现的。在这其中会涉及到指针的修改，在高并发的场景下，可能会使链表上的一个节点的下一个指针指向了其前一个节点，也就是形成了死循环

而Java8中通过形成两个链表，节点hash值在数组容量二进制数为1的那个位置处去按位与判断是0还是1，以此来选择插入的方式很好地解决了这个问题，而且不用每一个节点rehash，提高了执行速度。

 HashMap 本身就是非线程安全的，如果要在多线程下，建议使用 ConcurrentHashMap 替代.







#### 参考文献

[聊聊经典数据结构HashMap,逐行分析每一个关键点](https://baijiahao.baidu.com/s?id=1679219630514764243&wfr=spider&for=pc)

[详解 HashMap 中的 Hash 算法（扰动函数）](https://zhangzw.com/posts/20190925.html)

[终结HashMap面试？我是谁？我在哪](https://zhuanlan.zhihu.com/p/77899892)

[面试中最喜欢问的HashMap知识都在这里，赶紧收藏！](https://baijiahao.baidu.com/s?id=1662733945029523270&wfr=spider&for=pc)

[Java集合之一—HashMap](https://blog.csdn.net/woshimaxiao1/article/details/83661464)

java源码剖析34讲

[Hashmap实现原理及扩容机制详解](https://blog.csdn.net/lkforce/article/details/89521318)





### MySQL

#### 为什么要分库分表?

数据库出现性能瓶颈。对外表现有几个方面：

- 大量请求阻塞在高并发场景下，大量请求都需要操作数据库，导致连接数不够了，请求处于阻塞状态。
- SQL 操作变慢. 如果数据库中存在一张上亿数据量的表，一条 SQL 没有命中索引会全表扫描，这个查询耗时会非常久;
- 存储出现问题业务量剧增，单库数据量越来越大，给存储造成巨大压力。

从机器的角度看，性能瓶颈无非就是CPU、内存、磁盘、网络这些，要解决性能瓶颈最简单粗暴的办法就是提升机器性能，但是通过这种方法成本和收益投入比往往又太高了，不划算，所以重点还是要从软件角度入手。



#### 数据库相关优化方案

数据库优化方案很多，主要分为两大类：软件层面、硬件层面。

- 软件层面包括：
	- SQL 调优	
	- 表结构优化
	- 读写分离
	- 数据库集群
	- 分库分表等；

- 硬件层面主要是增加机器性能。

##### SQL 调优

SQL 调优往往是解决数据库问题的第一步，往往投入少部分精力就能获得较大的收益。

SQL 调优主要目的是尽可能地让那些慢 SQL 变快，手段其实也很简单就是让 SQL 执行尽量命中索引。

如果你使用的是 Mysql，需要在 Mysql 配置文件中配置几个参数即可

```ini
slow_query_log=on
long_query_time=1
slow_query_log_file=/path/to/log

```

常常会用到 explain 这个命令来查看 SQL 语句的执行计划，通过观察执行结果很容易就知道该 SQL 语句是不是全表扫描、有没有命中索引。

```mysql
-- 查看SQL是否使用索引，前面加上explain即可
explain select * from emp where name = 'abc';
```

![img](https://gitee.com/tadpole145/images/raw/main/20210928141028.png)

type列代表 表访问方式，表示MySQL在表中找到所需行的方式，又称“访问类型”。

常用的类型有： **ALL、index、range、 ref、eq_ref、const、system、 NULL**（从左到右，性能从差到好）

- ALL：  MySQL将遍历全表以找到匹配的行

- index: Full Index Scan，index与ALL区别为index类型只遍历索引树

- range:只检索给定范围的行，使用一个索引来选择行

- ref: 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值

- eq_ref: 类似ref，区别就在使用的索引是唯一索引，对于每个索引键值，表中只有一条记录匹配，简单来说，就是多表连接中使用primary key或者 unique key作为关联条件

- const、system: 当MySQL对查询某部分进行优化，并转换为一个常量时，使用这些类型访问。如将主键置于where列表中，MySQL就能将该查询转换为一个常量，system是const类型的特例，当查询的表只有一行的情况下，使用system

- NULL: MySQL在优化过程中分解语句，执行时甚至不用访问表或索引，例如从一个索引列里选取最小值可以通过单独索引查找完成。

##### 表结构优化

以一个场景举例说明：

“user”表中有 user_id、nickname 等字段，“order”表中有order_id、user_id等字段，如果想拿到用户昵称怎么办？一般情况是通过 join 关联表操作，在查询订单表时关联查询用户表，从而获取到用户昵称。

但是随着业务量增加，订单表和用户表肯定也是暴增，这时候通过两个表关联数据就比较费力了，为了取一个昵称字段而不得不关联查询几十上百万的用户表，其速度可想而知。

这个时候可以尝试将 nickname 这个字段加到 order 表中（order_id、user_id、nickname），这种做法通常叫做数据库表冗余字段。这样做的好处展示订单列表时不需要再关联查询用户表了。

冗余字段的做法也有一个弊端，如果这个字段更新会同时涉及到多个表的更新，因此在选择冗余字段时要尽量选择不经常更新的字段.

##### 架构优化

当单台数据库实例扛不住，我们可以增加实例组成集群对外服务。

当发现读请求明显多于写请求时，我们可以让主实例负责写，从实例对外提供读的能力；

如果读实例压力依然很大，可以在数据库前面加入缓存如 redis，让请求优先从缓存取数据减少数据库访问。

缓存分担了部分压力后，数据库依然是瓶颈，这个时候就可以考虑分库分表的方案了

##### 分表分库

###### 分库

当业务量上来后,单数据库的能够支撑的并发量是有限的，拆成多个库可以使服务间不用竞争，提升服务的性能。

###### 分表

当一个表增长过快,导致查询效率明显下降,就需要考虑分表了.

分表有几个维度，一是水平切分和垂直切分，二是单库内分表和多库内分表。

- 垂直拆分, 将一个表中不常用的字段拆分出去, 比如用户表可以拆分为用户基础信息表与用户详细信息表.   特点:  基于表或字段划分，表结构不同。
- 水平拆分, 按id固定行数进行水平拆分,还可以按照时间维度去拆分，比如订单表，可以按每日、每月等进行拆分.   特点: 基于数据划分，表结构相同，数据不同。

- 单库内分表, 每张表都拆分为了多个子表，多个子表存在于同一数据库中。一定程度上可以解决单表查询性能的问题，但是也会遇到一个问题：单数据库存储瓶颈。 比如: 商城秒杀时候用户都在刷新当前商品列表页面,会导致商品数据库访问量剧增,而用户数据库等没啥访问量,因此业界一般采用多库内分表.
- 多库内分表, 就是将一个表从一个数据库拆分到不同的数据库内,减轻单个数据库的访问压力.

##### 硬件优化

硬件成本非常高，一般来说不可能遇到数据库性能瓶颈就去升级硬件。

在前期业务量比较小的时候，升级硬件数据库性能可以得到较大提升；但是在后期，升级硬件得到的收益就不那么明显了。

#### 分库分表带来的复杂性

分库分表虽然在大量数据时候解决了数据访问性能, 但同时也会给系统带来很多复杂性,具体如下:

- 跨库关联查询

	单库单表时, 可以很方便的使用join操作关联多表查询数据,分库分表后,如何跨库查询?

	   - 字段冗余, 把需要关联的字段放入主表中，尽量避免 join 操作；
	   - 通过ETL等将数据汇合聚集，生成新的表；
	   - 全局表：比如一些基础表可以在每个数据库中都放一份；
	   - 应用层组装：将基础数据查出来，通过应用程序计算组装；

- 分布式事务

单数据库可以用本地事务搞定，使用多数据库就只能通过分布式事务解决了。

常用解决方案有：基于可靠消息（MQ）的解决方案、两阶段事务提交、柔性事务等

- 排序, 分页,函数计算问题

在使用 SQL 时 order by， limit 等关键字需要特殊处理，一般来说采用分片的思路：

先在每个分片上执行相应的函数，然后将各个分片的结果集进行汇总和再次计算，最终得到结果

- 分布式ID

	如果使用 Mysql 数据库在单库单表可以使用 id 自增作为主键，分库分表了之后就不行了，会出现id 重复。

	常用的分布式 ID 解决方案有：

	- UUID
	- Redis 缓存
	- 雪花算法（Snowflake）
	- 大厂三方方案.

- 多数据源

	分库分表之后可能会面临从多个数据库或多个子表中获取数据，一般的解决思路有：客户端适配和代理层适配。业界常用的中间件有：

	- shardingsphere（前身 sharding-jdbc）
	- Mycat



### 锁

#### 悲观锁

总是假设最坏的情况,每次拿数据的时候,都认为别人会修改,所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁. 

传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中 synchronized 和 ReentrantLock 等独占锁就是悲观锁思想的实现。

##### 缺点

线程阻塞,效率低下

#### 乐观锁

总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。

乐观锁适用于==多读==的应用类型, 这样可以提高吞吐量. 像数据库提供的类似于write_condition机制, 就属于乐观锁.在Java中 java.util.concurrent.atomic 包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。

##### 常见实现方式

###### 版本号机制

一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加1。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。

######  CAS算法

即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步。CAS算法涉及到三个操作数

- 需要读写的内存值 V 
- 进行比较的值 A 
- 拟写入的新值 B

当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。

##### 缺点

###### ABA问题

如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回A，那CAS操作就会误认为它从来没有被修改过。这个问题被称为CAS操作的 "ABA"问题。

JDK 1.5 以后的  AtomicStampedReference 类 就提供了此种能力，其中的  compareAndSet 方法 就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。

总的来说,就是在原来值相等的基础上,增加了一个是否修改过的标志.2个都相等才会执行更新操作.

###### 循环时间长,开销大

自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。 如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline ﬂush），从而提高CPU的执行效率。

######  只能保证一个共享变量的原子操作

CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了AtomicReference类 来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用 AtomicReference类 把多个共享变量合并成一个共享变量来操作。

#### 使用场景介绍

从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。

但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样
反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。

#####  CAS与synchronized的使用情景

- 对于资源竞争较少（线程冲突较轻）的情况，使用synchronized同步锁进行线程阻塞和唤醒切换以及用户态内核态间的切换操作额外浪费消耗cpu资源；而CAS基于硬件实现，不需要进入内核，不需要切换线程，操作自旋几率较少，因此可以获得更高的性能。
-  对于资源竞争严重（线程冲突严重）的情况，CAS自旋的概率会比较大，从而浪费更多的CPU资源，效率低于synchronized。

### AQS介绍

AQS 的全称为（AbstractQueuedSynchronizer），这个类在 java.util.concurrent.locks 包下面。AQS 是一个用来构建锁和同步器的框架，使用 AQS 能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的 ReentrantLock，Semaphore，其他的诸如 ReentrantReadWriteLock，
SynchronousQueue，FutureTask(jdk1.7) 等等皆是基于 AQS 的。当然，我们自己也能利用 AQS 非常轻松容易地构造出符合我们自己需求的同步器。

####  AQS 原理

AQS 核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制 AQS 是用 CLH 队列锁实现的，即将暂时获取不到锁的线程加入到队列中。

CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS 是将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点（Node）来实现锁的分配。

![image-20210928164909180](https://gitee.com/tadpole145/images/raw/main/20210928164909.png)

AQS 使用一个 int 成员变量来表示同步状态，通过内置的 FIFO 队列来完成获取资源线程的排队工作。AQS 使用 CAS 对该同步状态进行原子操作实现对其值的修改。

####  AQS 对资源的共享方式

1.  Exclusive（独占）;

	只有一个线程能执行，如 ReentrantLock。又可分为公平锁和非公平锁,ReentrantLock 同时支持两种锁,下面以 ReentrantLock 对这两种锁的定义做介绍：

	- 公平锁：按照线程在队列中的排队顺序，先到者先拿到锁
	- 非公平锁：当线程要获取锁时，先通过两次 CAS 操作去抢锁，如果没抢到，当前线程再加入到队列中等待唤醒。

ReentrantLock 默认采用非公平锁，因为考虑获得更好的性能，通过 boolean 来决定是否用公平锁（传入 true 用公平锁）。

- 非公平锁在调用 lock 后，首先就会调用 CAS 进行一次抢锁，如果这个时候恰巧锁没有被占用，那么直接就获取到锁返回了。
- 非公平锁在 CAS 失败后，和公平锁一样都会进入到 tryAcquire 方法，在 tryAcquire 方法中，如果发现锁这个时候被释放了（state == 0），非公平锁会直接 CAS 抢锁，但是公平锁会判断等待队列是否有线程处于等待状态，如果有则不去抢锁，乖乖排到后面。

，如果这两次 CAS 都不成功，那么后面非公平锁和公平锁是一样的，都要进入到阻塞队列等待唤醒。

2. Share（共享）

多个线程可同时执行，如 Semaphore、CountDownLatch、 CyclicBarrier、ReadWriteLock等.

不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源 state 的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出等），AQS 已经在上层已经帮我们实现好了。

AQS 使用了模板方法模式，自定义同步器时需要重写下面几个 AQS 提供的模板方法：

```java
isHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。
tryAcquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。
tryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。
tryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。
tryReleaseShared(int)//共享方式。尝试释放资源，成功则返回true，失败则返回false。
```

默认情况下，每个方法都抛出  UnsupportedOperationException 。 这些方法的实现必须是内部线程安全的，并且通常应该简短而不是阻塞。AQS 类中的其他方法都是 ﬁnal ，所以无法被其他类使用，只有这几个方法可以被其他类使用。

以 ReentrantLock 为例，state 初始化为 0，表示未锁定状态。A 线程 lock()时，会调用 tryAcquire()独占该锁并将 state+1。此后，其他线程再 tryAcquire()时就会失败，直到 A 线程 unlock()到 state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A 线程自己是可以重复获取此锁的（state 会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证 state 是能回到零态的。

#### Semaphore(信号量)-允许多个线程同时访问

synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore(信号量)可以指定多个线程同时访问某个资源。 

Semaphore 有两种模式，公平模式和非公平模式。

- 公平模式： 调用 acquire 的顺序就是获取许可证的顺序，遵循 FIFO；
- 非公平模式： 抢占式的。
